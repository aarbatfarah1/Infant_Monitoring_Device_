{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eca2788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chaimaa\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac06ca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 317ms/step\n",
      "The audio is classified as: hungry\n"
     ]
    }
   ],
   "source": [
    "audio_path_to_test = 'baby-crying-01.wav'\n",
    "def predict(audio) :\n",
    "    # Load the saved model\n",
    "    model = load_model('rnn_mfcc_model.h5')\n",
    "    min_shape= 281\n",
    "    # Modify the list of classes\n",
    "    classes = ['belly_pain', 'burping', 'discomfort', 'hungry', 'tired']\n",
    "\n",
    "\n",
    "    # Function to extract audio features (MFCC)\n",
    "    def extract_features(audio_path):\n",
    "        audio, _ = librosa.load(audio_path, res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)\n",
    "        return mfccs\n",
    "\n",
    "    # Function to reshape and truncate the MFCCs\n",
    "    def reshape_and_truncate(mfccs, target_shape):\n",
    "        if mfccs.shape[1] > target_shape:\n",
    "            return mfccs[:, :target_shape]\n",
    "        else:\n",
    "            return np.pad(mfccs, ((0, 0), (0, target_shape - mfccs.shape[1])))\n",
    "\n",
    "    # Function to process audio file and make a prediction\n",
    "    def predict_audio_class(audio_path, model, min_shape):\n",
    "        # Extract features of the new audio\n",
    "        new_audio_features = extract_features(audio_path)\n",
    "\n",
    "        # Ensure features have the same shape as those used for training\n",
    "        new_audio_features = reshape_and_truncate(new_audio_features, min_shape)\n",
    "\n",
    "        # Add an additional dimension for the batch size\n",
    "        new_audio_features = np.expand_dims(new_audio_features, axis=0)\n",
    "\n",
    "        # Predict the class of the new audio\n",
    "        predictions = model.predict(new_audio_features)\n",
    "\n",
    "        # Get the class probabilities\n",
    "        class_probabilities = predictions[0]\n",
    "\n",
    "        # Get the predicted class index\n",
    "        predicted_class_index = np.argmax(class_probabilities)\n",
    "\n",
    "        return class_probabilities, predicted_class_index\n",
    "\n",
    "    \n",
    "\n",
    "    # Make a prediction for the new audio\n",
    "    class_probabilities, predicted_class_index = predict_audio_class(audio, model, min_shape)\n",
    "\n",
    "    # Calculate and display the predicted class and accuracy\n",
    "    predicted_class = classes[predicted_class_index]\n",
    "    accuracy = class_probabilities[predicted_class_index]\n",
    "    return predicted_class\n",
    "predicted_class=predict(audio_path_to_test)\n",
    "print(f'The audio is classified as: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495399dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
