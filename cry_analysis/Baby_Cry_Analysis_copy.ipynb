{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Définir le chemin vers votre dataset\n",
    "path_to_your_dataset = 'donateacry_corpus_cleaned_and_updated_data'\n",
    "\n",
    "# Listez les sous-dossiers correspondant à chaque classe\n",
    "subfolders = ['belly_pain', 'burping', 'discomfort', 'hungry', 'tired']\n",
    "\n",
    "# Initialiser des listes pour stocker les chemins des fichiers audio et les étiquettes\n",
    "data = []    # Cette liste stockera les chemins des fichiers audio\n",
    "labels = []  # Cette liste stockera les étiquettes correspondantes (noms de classe)\n",
    "\n",
    "# Parcourez chaque sous-dossier\n",
    "for label, subfolder in enumerate(subfolders):\n",
    "    folder_path = os.path.join(path_to_your_dataset, subfolder)\n",
    "    \n",
    "    # Parcourez chaque fichier audio dans le sous-dossier\n",
    "    for audio_file in os.listdir(folder_path):\n",
    "        if audio_file.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(folder_path, audio_file)\n",
    "            data.append(audio_path)\n",
    "            labels.append(subfolder)\n",
    "\n",
    "# Convertissez les étiquettes en nombres\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Divisez les données en ensembles de formation et de test (80% formation, 20% test)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fonction pour normaliser les fichiers audio\n",
    "def normalize_audio(audio_path):\n",
    "    audio, _ = librosa.load(audio_path, res_type='kaiser_fast')\n",
    "    normalized_audio = librosa.util.normalize(audio)\n",
    "    return normalized_audio\n",
    "\n",
    "# Normalisez les fichiers audio dans les ensembles de formation et de test\n",
    "train_data = [normalize_audio(audio_path) for audio_path in train_data]\n",
    "test_data = [normalize_audio(audio_path) for audio_path in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les caractéristiques audio (spectrogrammes)\n",
    "def extract_spectrogram(audio):\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=22050, n_mels=128, fmax=8000)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    return spectrogram_db\n",
    "\n",
    "# Créez des ensembles de formation et de test de spectrogrammes\n",
    "X_train = [extract_spectrogram(audio) for audio in train_data]\n",
    "X_test = [extract_spectrogram(audio) for audio in test_data]\n",
    "\n",
    "# Trouvez la forme minimale des spectrogrammes\n",
    "min_shape = min([spectrogram.shape[1] for spectrogram in X_train + X_test])\n",
    "\n",
    "# Fonction pour remodeler et tronquer les spectrogrammes\n",
    "def reshape_and_truncate(spectrogram, target_shape):\n",
    "    if spectrogram.shape[1] > target_shape:\n",
    "        return spectrogram[:, :target_shape]\n",
    "    else:\n",
    "        return np.pad(spectrogram, ((0, 0), (0, target_shape - spectrogram.shape[1])))\n",
    "\n",
    "# Appliquer reshape_and_truncate à tous les spectrogrammes\n",
    "X_train = [reshape_and_truncate(spectrogram, min_shape) for spectrogram in X_train]\n",
    "X_test = [reshape_and_truncate(spectrogram, min_shape) for spectrogram in X_test]\n",
    "\n",
    "# Convertir les listes en tableaux NumPy\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "12/12 [==============================] - 6s 359ms/step - loss: 478.5129 - accuracy: 0.6192 - val_loss: 859.5894 - val_accuracy: 0.7826\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 4s 315ms/step - loss: 424.6591 - accuracy: 0.7808 - val_loss: 280.5923 - val_accuracy: 0.7826\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 4s 315ms/step - loss: 124.9874 - accuracy: 0.7260 - val_loss: 128.7720 - val_accuracy: 0.7826\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 4s 309ms/step - loss: 59.4088 - accuracy: 0.8192 - val_loss: 43.4251 - val_accuracy: 0.7826\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 4s 314ms/step - loss: 8.9866 - accuracy: 0.8110 - val_loss: 15.2428 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 4s 319ms/step - loss: 2.4972 - accuracy: 0.8712 - val_loss: 11.7944 - val_accuracy: 0.7826\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 4s 313ms/step - loss: 1.6252 - accuracy: 0.9068 - val_loss: 9.7831 - val_accuracy: 0.7826\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 4s 314ms/step - loss: 0.2346 - accuracy: 0.9644 - val_loss: 6.0340 - val_accuracy: 0.6739\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 4s 341ms/step - loss: 0.0704 - accuracy: 0.9863 - val_loss: 7.8529 - val_accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 4s 319ms/step - loss: 0.0286 - accuracy: 0.9945 - val_loss: 7.9915 - val_accuracy: 0.7826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x186c3313b90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Définissez l'architecture du modèle\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(len(subfolders), activation='softmax'))\n",
    "\n",
    "# Compilez le modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Ajoutez une dimension pour indiquer les canaux (1 pour les niveaux de gris)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "# Entraînez le modèle\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 39ms/step - loss: 7.9915 - accuracy: 0.7826\n",
      "Test Accuracy: 0.782608687877655\n",
      "3/3 [==============================] - 0s 42ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  belly_pain       0.00      0.00      0.00         4\n",
      "     burping       0.00      0.00      0.00         2\n",
      "  discomfort       0.00      0.00      0.00         7\n",
      "      hungry       0.80      1.00      0.89        72\n",
      "       tired       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.78        92\n",
      "   macro avg       0.16      0.20      0.18        92\n",
      "weighted avg       0.63      0.78      0.70        92\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Évaluez le modèle sur l'ensemble de test\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Faites des prédictions sur l'ensemble de test\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convertissez les prédictions en classes\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Analysez les métriques telles que la précision, le rappel, et la précision\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_classes, target_names=subfolders))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\\\\\Users\\\\Dell\\\\\\\\ENSIAS\\\\S3\\\\\\\\Infant_Monitoring_Device_\\\\\\\\cry_analysis\\\\donateacry_corpus_cleaned_and_updated_data\\\\belly_pain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m     input_audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, audio_file)\n\u001b[0;32m     36\u001b[0m     output_audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, audio_file)\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_audio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_audio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Function to generate a unique hash for an audio file\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_audio_hash\u001b[39m(audio_data):\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\shutil.py:419\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[0;32m    418\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[1;32m--> 419\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\shutil.py:256\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    254\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m    259\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\\\\\Users\\\\Dell\\\\\\\\ENSIAS\\\\S3\\\\\\\\Infant_Monitoring_Device_\\\\\\\\cry_analysis\\\\donateacry_corpus_cleaned_and_updated_data\\\\belly_pain'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "import shutil\n",
    "import hashlib\n",
    "\n",
    "input_folder = r\"C:\\\\Users\\Dell\\\\ENSIAS\\S3\\\\Infant_Monitoring_Device_\\\\cry_analysis\\donateacry_corpus_cleaned_and_updated_data\"  # Replace with the actual path to your input folder\n",
    "output_folder = r\"C:\\\\Users\\Dell\\\\ENSIAS\\S3\\\\Infant_Monitoring_Device_\\\\cry_analysis\\donateacry_corpus_cleaned_and_updated_data_augmentated\"  # Replace with the actual path to your output folder\n",
    "\n",
    "desired_num_audios = 10  # number of desired audios\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "crop_duration = 3  # Random cropping duration in seconds\n",
    "noise_level = 0.05  # Standard deviation for random normal distribution noise\n",
    "volume_factor_range = [0.8, 1.2]  # Range for volume adjustment\n",
    "speed_factor_range = [0.8, 1.2]  # Range for speed perturbation\n",
    "pitch_semitones_range = [-2, 2]  # Range for pitch variation\n",
    "\n",
    "# Define a set to store unique hashes of generated audio files\n",
    "generated_audio_hashes = set()\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "audio_files = os.listdir(input_folder)\n",
    "num_input_audios = len(audio_files)\n",
    "\n",
    "# Copy the original audios\n",
    "for audio_file in audio_files:\n",
    "    input_audio_path = os.path.join(input_folder, audio_file)\n",
    "    output_audio_path = os.path.join(output_folder, audio_file)\n",
    "    shutil.copy(input_audio_path, output_audio_path)\n",
    "\n",
    "# Function to generate a unique hash for an audio file\n",
    "def generate_audio_hash(audio_data):\n",
    "    return hashlib.sha1(audio_data).hexdigest()\n",
    "\n",
    "# Augment the audio files\n",
    "generated_audios = 0\n",
    "while generated_audios < desired_num_audios:\n",
    "    input_audio_file = random.choice(audio_files)\n",
    "    input_audio_path = os.path.join(input_folder, input_audio_file)\n",
    "\n",
    "    original_audio, sr = librosa.load(input_audio_path, sr=None)\n",
    "\n",
    "    # Random cropping\n",
    "    crop_start = np.random.uniform(0, len(original_audio) - crop_duration * sr)\n",
    "    cropped_audio = original_audio[int(crop_start):int(crop_start + crop_duration * sr)]\n",
    "\n",
    "    # Generate a hash for the augmented audio\n",
    "    audio_hash = generate_audio_hash(cropped_audio)\n",
    "\n",
    "    # Check if the hash is already in the set (duplicate)\n",
    "    if audio_hash not in generated_audio_hashes:\n",
    "        augmented_audio = cropped_audio.copy()\n",
    "        output_audio_path = os.path.join(output_folder, f\"random_cropping_{generated_audios + 1}.wav\")\n",
    "        wavfile.write(output_audio_path, sr, augmented_audio.astype(np.float32))\n",
    "        print(f\"Random cropping audio {generated_audios + 1}/{desired_num_audios} saved: {output_audio_path}\")\n",
    "\n",
    "        # Add the hash to the set\n",
    "        generated_audio_hashes.add(audio_hash)\n",
    "\n",
    "        generated_audios += 1\n",
    "\n",
    "    # Continue to the next iteration if duplicates are found\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Noise injection\n",
    "    noise = np.random.normal(0, noise_level, len(cropped_audio))\n",
    "    augmented_audio = cropped_audio + noise\n",
    "    audio_hash = generate_audio_hash(augmented_audio)\n",
    "    if audio_hash not in generated_audio_hashes:\n",
    "        output_audio_path = os.path.join(output_folder, f\"noise_injection_{generated_audios + 1}.wav\")\n",
    "        wavfile.write(output_audio_path, sr, augmented_audio.astype(np.float32))\n",
    "        print(f\"Noise injection audio {generated_audios + 1}/{desired_num_audios} saved: {output_audio_path}\")\n",
    "        generated_audio_hashes.add(audio_hash)\n",
    "        generated_audios += 1\n",
    "\n",
    "    # Volume adjustment\n",
    "    volume_factor = np.random.uniform(*volume_factor_range)\n",
    "    augmented_audio = cropped_audio * volume_factor\n",
    "    audio_hash = generate_audio_hash(augmented_audio)\n",
    "    if audio_hash not in generated_audio_hashes:\n",
    "        output_audio_path = os.path.join(output_folder, f\"volume_adjustment_{generated_audios + 1}.wav\")\n",
    "        wavfile.write(output_audio_path, sr, augmented_audio.astype(np.float32))\n",
    "        print(f\"Volume adjustment audio {generated_audios + 1}/{desired_num_audios} saved: {output_audio_path}\")\n",
    "        generated_audio_hashes.add(audio_hash)\n",
    "        generated_audios += 1\n",
    "\n",
    "    # Speed perturbation\n",
    "    speed_factor = np.random.uniform(*speed_factor_range)\n",
    "    augmented_audio = librosa.effects.time_stretch(cropped_audio, speed_factor)\n",
    "    audio_hash = generate_audio_hash(augmented_audio)\n",
    "    if audio_hash not in generated_audio_hashes:\n",
    "        output_audio_path = os.path.join(output_folder, f\"speed_perturbation_{generated_audios + 1}.wav\")\n",
    "        wavfile.write(output_audio_path, sr, augmented_audio.astype(np.float32))\n",
    "        print(f\"Speed perturbation audio {generated_audios + 1}/{desired_num_audios} saved: {output_audio_path}\")\n",
    "        generated_audio_hashes.add(audio_hash)\n",
    "        generated_audios += 1\n",
    "\n",
    "    # Pitch variation\n",
    "    pitch_semitones = np.random.uniform(*pitch_semitones_range)\n",
    "    augmented_audio = librosa.effects.pitch_shift(cropped_audio, sr, pitch_semitones)\n",
    "    audio_hash = generate_audio_hash(augmented_audio)\n",
    "    if audio_hash not in generated_audio_hashes:\n",
    "        output_audio_path = os.path.join(output_folder, f\"pitch_variation_{generated_audios + 1}.wav\")\n",
    "        wavfile.write(output_audio_path, sr, augmented_audio.astype(np.float32))\n",
    "        print(f\"Pitch variation audio {generated_audios + 1}/{desired_num_audios} saved: {output_audio_path}\")\n",
    "        generated_audio_hashes.add(audio_hash)\n",
    "        generated_audios += 1\n",
    "\n",
    "    # Continue to the next iteration if duplicates are found\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 43ms/step - loss: 0.8522 - accuracy: 0.7661 - val_loss: 0.7191 - val_accuracy: 0.8142\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.5304 - accuracy: 0.8536 - val_loss: 0.5948 - val_accuracy: 0.8251\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.4293 - accuracy: 0.8632 - val_loss: 0.5283 - val_accuracy: 0.8306\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3616 - accuracy: 0.8769 - val_loss: 0.4859 - val_accuracy: 0.8361\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3073 - accuracy: 0.8906 - val_loss: 0.4639 - val_accuracy: 0.8634\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.2706 - accuracy: 0.9001 - val_loss: 0.4259 - val_accuracy: 0.8743\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.2263 - accuracy: 0.9138 - val_loss: 0.4048 - val_accuracy: 0.8798\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1974 - accuracy: 0.9289 - val_loss: 0.3796 - val_accuracy: 0.8689\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1678 - accuracy: 0.9412 - val_loss: 0.3620 - val_accuracy: 0.8852\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.1487 - accuracy: 0.9494 - val_loss: 0.3413 - val_accuracy: 0.9071\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3413 - accuracy: 0.9071\n",
      "Test Accuracy: 0.9071038365364075\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each subfolder\n",
    "for label, subfolder in enumerate(subfolders):\n",
    "    folder_path = os.path.join(path_to_your_dataset, subfolder)\n",
    "    \n",
    "    # Iterate through each audio file in the subfolder\n",
    "    for audio_file in os.listdir(folder_path):\n",
    "        if audio_file.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(folder_path, audio_file)\n",
    "            data.append(audio_path)\n",
    "            labels.append(subfolder)\n",
    "\n",
    "# Convert labels to numbers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to extract audio features (MFCCs)\n",
    "def extract_features(audio_path):\n",
    "    audio, _ = librosa.load(audio_path, res_type='kaiser_fast')\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)\n",
    "    return mfccs\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train = [extract_features(audio_path) for audio_path in train_data]\n",
    "X_test = [extract_features(audio_path) for audio_path in test_data]\n",
    "\n",
    "# Find the minimum shape of the MFCCs\n",
    "min_shape = min([mfccs.shape[1] for mfccs in X_train + X_test])\n",
    "\n",
    "# Function to reshape and truncate the MFCCs\n",
    "def reshape_and_truncate(mfccs, target_shape):\n",
    "    if mfccs.shape[1] > target_shape:\n",
    "        return mfccs[:, :target_shape]\n",
    "    else:\n",
    "        return np.pad(mfccs, ((0, 0), (0, target_shape - mfccs.shape[1])))\n",
    "\n",
    "# Apply reshape_and_truncate to all MFCCs\n",
    "X_train = [reshape_and_truncate(mfccs, min_shape) for mfccs in X_train]\n",
    "X_test = [reshape_and_truncate(mfccs, min_shape) for mfccs in X_test]\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Reshape the input data for LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Create the LSTM model\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(layers.Dense(len(subfolders), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_one_hot, epochs=10, validation_data=(X_test, y_test_one_hot))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
